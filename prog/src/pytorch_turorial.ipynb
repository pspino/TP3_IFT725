{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutoriel pytorch - TP3 - IFT725\n",
    "\n",
    "Tel que mentionné dans l'énoncé du travail, vous devez recopier les blocs de code du tutoriel suivant\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "\n",
    "en donnant, pour chaque bloc, une description en format \"markdown\" de son contenu."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenseurs\n",
    "## Avec Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size, Din is input dimension, \n",
    "# H is hidden layer dimension and Dout is output dimension\n",
    "N, Din, H, Dout = 64, 1000, 100, 10\n",
    "\n",
    "# Create some random data.\n",
    "x = np.random.randn(N, Din)\n",
    "y = np.random.randn(N, Dout)\n",
    "\n",
    "w1 = np.random.randn(Din, H)\n",
    "w2 = np.random.randn(H, Dout)\n",
    "\n",
    "lr = 1e-6\n",
    "for e in range(500):\n",
    "    # Forward Pass\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute data\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(e, loss)\n",
    "\n",
    "    # Backward Pass\n",
    "    grad_y_pred = 2.0* (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Weight update\n",
    "    w1 -= lr * grad_w1\n",
    "    w2 -= lr * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ce bloc permet de créer un réseau avec une seule couche pleinement connectée asssociée à fonction d'activation ReLU.\n",
    "\n",
    "Pour cela, on s'intéresse à plusieurs variables essentielles:\n",
    "x est un ndarray contenant les données d'entrée du modèle, dont le ndarray y représente ses labels.\n",
    "w1 et w2 contiennent les poids que le modèle apprend, initialisés aléatoirement. Ceux sont également des ndarrays, qui \n",
    "sont mis à jour à chaque epoch.\n",
    "Le taux d'erreur du modèle est contenu dans la variable loss.\n",
    "grad_w1 et grad_w2 sont les résultats des calculs des gradients des poids du réseau."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Via Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")       # gpu = cuda\n",
    "\n",
    "# N is batch size, Din is input dimension, \n",
    "# H is hidden layer dimension and Dout is output dimension\n",
    "N, Din, H, Dout = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, Din, device=device, dtype=dtype)\n",
    "y = torch.randn(N, Dout, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(Din, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, Dout, device=device, dtype=dtype)\n",
    "\n",
    "lr = 1e-6\n",
    "for e in range(500):\n",
    "    # Forward pass \n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if e % 100 == 0:\n",
    "        print(e, loss)\n",
    "\n",
    "    # Backward pass\n",
    "    grad_y_pred = 2.0* (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Weight update\n",
    "    w1 -= lr * grad_w1\n",
    "    w2 -= lr * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ce bloc permet de créer le même réseau que précédemment seulement au lieu d'utiliser des variables ndarrays ceux sont \n",
    "des tenseurs Pytorch. Ceux-ci ont quasiment la même structure qu'un ndarray mais permettent d'utiliser les ressources\n",
    "d'un GPU dans le but d'accélérer les opérations matricielles."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograd\n",
    "## Tenseurs et autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")       # gpu = cuda\n",
    "\n",
    "# N is batch size, Din is input dimension, \n",
    "# H is hidden layer dimension and Dout is output dimension\n",
    "N, Din, H, Dout = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, Din, device=device, dtype=dtype)\n",
    "y = torch.randn(N, Dout, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(Din, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, Dout, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "lr = 1e-6 \n",
    "for e in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if e % 100 == 0:\n",
    "        print(e, loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w1 -= lr* w1.grad\n",
    "        w2 -= lr* w2.grad\n",
    "\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ce bloc utilise le package autograd, qui permet d'utiliser un graphe computationnel. Celui-ci est créé à la ligne\n",
    "138, à la volée. Afin que les gradients des poids du réseau soient calculés lors de la rétro-propagation, on indique \n",
    "lors de l'initialisation des variables w_1 et w_2 qu'il est requis de sauvgarder leur gradient. \n",
    "La méthode backward permet de calculer la rétro rétropropagation et dès lors, le graphe créé précédemment est éliminé de\n",
    " la mémoire. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PyTorch: Définition de nouvelles fonctions autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(context, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        context.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(context, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = context.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")       # gpu = cuda\n",
    "\n",
    "# N is batch size, Din is input dimension, \n",
    "# H is hidden layer dimension and Dout is output dimension\n",
    "N, Din, H, Dout = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, Din, device=device, dtype=dtype)\n",
    "y = torch.randn(N, Dout, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(Din, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, Dout, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "lr = 1e-6 \n",
    "for e in range(500):\n",
    "    relu = MyReLU.apply\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if e % 100 == 0:\n",
    "        print(e, loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w1 -= lr * w1.grad\n",
    "        w2 -= lr * w2.grad\n",
    "\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ce bloc définit tout d'abord une classe MyReLU permettant d'implémenter des fonctions propres autograd, forward et \n",
    "backward. \n",
    "La variable relu contient une instance de cette classe. La ligne 217 applique la méthode forward pour chacun des noeuds\n",
    "du graphe. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn module\n",
    "## PyTorch: nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import *\n",
    "\n",
    "# N is batch size, Din is input dimension, \n",
    "# H is hidden layer dimension and Dout is output dimension\n",
    "N, Din, H, Dout = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, Din, device=device, dtype=dtype)\n",
    "y = torch.randn(N, Dout, device=device, dtype=dtype)\n",
    "\n",
    "criterion = MSELoss(reduction='sum')\n",
    "\n",
    "# Sequential is a module which contains other modules, and applies them in sequence\n",
    "# to produce its output. Each Linear Module is a Fully Connected Layer and holds \n",
    "# internal Tensor for its weights and bias.\n",
    "model = Sequential(\n",
    "    Linear(Din, H),\n",
    "    ReLU(),\n",
    "    Linear(H,Dout)\n",
    ")\n",
    "\n",
    "lr = 1e-6 \n",
    "for e in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    if e % 100 == 0:\n",
    "        print(e, loss.item())\n",
    "    \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le package nn permet de disposer de différents Modules, notamment des couches du réseau. La variable model contient\n",
    "donc plusieurs plusieurs Modules, possédant pour chacun d'eux leur propre implémentation de méthodes forward et backward\n",
    ". Passer en paramètre des données d'entrée x à un Module couche effectue une propagation avant.\n",
    "La variable criterion quant à elle est également un Module permettant de calculer l'erreur quadratique d'un modèle. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PyTorch: optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 700.7799682617188\n100 683.9097290039062\n200 667.51123046875\n300 651.6246337890625\n400 636.155029296875\n"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import *\n",
    "\n",
    "# N is batch size, Din is input dimension, \n",
    "# H is hidden layer dimension and Dout is output dimension\n",
    "N, Din, H, Dout = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, Din, device=device, dtype=dtype)\n",
    "y = torch.randn(N, Dout, device=device, dtype=dtype)\n",
    "\n",
    "lr = 1e-6\n",
    "criterion = MSELoss(reduction='sum')\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for e in range(500):\n",
    "    # Forward pass \n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        print(e, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ce bloc implémente l'utilisation du package optim permettant d'utiliser des méthodes d'optimisation de réseaux. \n",
    "La variable optimizer contient donc un type d'optimiseur qu'on applique sur les poids du réseau en appelant la méthode\n",
    "step."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Custom nn Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 639.8817138671875\n100 639.8817138671875\n200 639.8817138671875\n300 639.8817138671875\n400 639.8817138671875\n"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import *\n",
    "from torch.optim import SGD\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__ (self, Din, H, Dout):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = Linear(Din, H)\n",
    "        self.linear2 = Linear(H, Dout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        return self.linear2(h_relu)\n",
    "    \n",
    "# N is batch size, Din is input dimension, \n",
    "# H is hidden layer dimension and Dout is output dimension\n",
    "N, Din, H, Dout = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, Din, device=device, dtype=dtype)\n",
    "y = torch.randn(N, Dout, device=device, dtype=dtype)\n",
    "\n",
    "lr = 1e-6\n",
    "criterion = MSELoss(reduction='sum')\n",
    "optimizer = SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model = TwoLayerNet(Din, H, Dout)\n",
    "\n",
    "for e in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    if e % 100 == 0:\n",
    "        print(e, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ce bloc implémente un réseau deux couches pleinement connectées prenant en entrée un Module. Cette classe \n",
    "réécrit les méthodes forward et backward du Module.         "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Control Flow + Weight Sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 615.9288330078125\n100 663.4950561523438\n200 663.4950561523438\n300 624.8159790039062\n400 615.9288330078125\n"
    }
   ],
   "source": [
    "import random\n",
    "import torch \n",
    "\n",
    "from torch.nn import *\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, Din, H, Dout):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = Linear(Din, H)\n",
    "        self.middle_linear = Linear(H, H)\n",
    "        self.output_linear = Linear(H, Dout)\n",
    "\n",
    "    def forward (self, x):\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0,3)):\n",
    "             h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        return self.output_linear(h_relu)\n",
    "\n",
    "# N is batch size, Din is input dimension, \n",
    "# H is hidden layer dimension and Dout is output dimension\n",
    "N, Din, H, Dout = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, Din, device=device, dtype=dtype)\n",
    "y = torch.randn(N, Dout, device=device, dtype=dtype)\n",
    "\n",
    "lr = 1e-6\n",
    "criterion = MSELoss(reduction='sum')\n",
    "optimizer = SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model = DynamicNet(Din, H, Dout)\n",
    "\n",
    "for e in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    if e % 100 == 0:\n",
    "        print(e, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ce bloc implémente une classe DynamicNet qui permet de faire du partage des paramètres de poids du réseau grâce à \n",
    "sa réécriture de la méthode forward.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}